{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b86a684c",
   "metadata": {},
   "source": [
    "# 3. Creating an image classifier\n",
    "\n",
    "A classic machine learning task is to train a model to classify images. Over the next few weeks, you will go through the basic steps in the machine learning pipeline that you will need to build more complex models:\n",
    "1. Data handling\n",
    "2. Using Pytorch (a machine learning framework) to define models\n",
    "3. Create a training loop\n",
    "4. Metric logging\n",
    "5. Pytorch Lightning\n",
    "\n",
    "For this exercise, we will be training a neural network to identify the digits 0 - 9 from pictures of handwritten numbers. In other words, given a picture of a digit such as this:  \n",
    "<img style=\"float: left;\" src=\"img/seven.png\"/> \n",
    "<br clear=\"left\"/>\n",
    "Our model should be able to predict that this is the digit 7. Even relatively simple models (such as the one we will be building) can achieve very good results, as we will see."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "936afb51",
   "metadata": {},
   "source": [
    "## 3.1 Pytorch\n",
    "Pytorch is a framework that provides many convenience functions and classes that will help us build our machine learning network much faster than we would be able to from scratch using pure Python. It takes care of abstracting away the components of your neural network (called `Module`s) and making sure that things like forward and backwards passes, gradient calculations, and weight updates are handled for you. It also facilitates the loading of data via classes like `Dataset` and `Dataloader`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f353850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd2d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abb346d9",
   "metadata": {},
   "source": [
    "### 3.1.1  The MNIST dataset\n",
    "The starting point of any machine learning project is the data. We need to ensure that we have a suitable dataset for the task at hand, and we need to do some work in order to prepare it before it is eventually put into our model. The dataset we will be using for our number recognition task is the popular [MNIST dataset](http://yann.lecun.com/exdb/mnist/), which contains tens of thousands of images of digits written by different people and nicely formatted into a standard size (28 * 28 pixels).  \n",
    "\n",
    "Let's download this dataset using the special `!` operator that we saw in last week's lab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dfb168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist_url = ''   we already downloaded the data\n",
    "\n",
    "# download mnist dataset and place in current directory\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf400497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract contents\n",
    "#!tar -vxzf data/MNIST/raw.tar.gz -C data/MNIST  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd42b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv file that maps images to their labels\n",
    "#bash code :\n",
    "#!chmod +x data/MNIST/create_csv.sh && ./data/MNIST/create_csv.sh ./data/MNIST/raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41d1433d",
   "metadata": {},
   "source": [
    "### 3.1.2 Visualising Images\n",
    "When working with image data, it can be helpful to define visualisation helper functions to make sure that the data visually \"looks right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14040c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1df66e8e890>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def load_image_tensor(filepath, threshold):\n",
    "    img = Image.open(filepath)\n",
    "    transform = transforms.PILToTensor()\n",
    "    img_tensor = transform(img)\n",
    "    img_tensor = torch.where(img_tensor < threshold , 0, img_tensor)\n",
    "    return img_tensor\n",
    "\n",
    "img = load_image_tensor(\"data/MNIST/raw/6/img_202.jpg\",threshold=50).reshape(28,28)\n",
    "img.shape\n",
    "plt.imshow(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57c36862",
   "metadata": {},
   "source": [
    "### 3.1.2 Subclassing the Dataset class\n",
    "Dataset is an abstract class provided by Pytorch, which means it defines a specification of what its subclasses must look like via function signatures. However, these functions are not actually implemented in the source code of the Dataset class. It's the programmer's job (yours!) to make sure the methods laid out in the signatures are implemented. Let's import these modules now, along with some other useful packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ad2d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e68c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our dataset - it is a map-style dataset, which means we have to know its size at initialisation and  \n",
    "# must be able to access data points at arbitrary locations. This is why the following methods must be implemented:\n",
    "# __len__ and __getitem__ \n",
    "class MNISTDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath: str): \n",
    "        super().__init__()\n",
    "        # Load data from CSV filepath defined earlier into a Pandas dataframe\n",
    "        self.dataset = pd.read_csv(filepath)\n",
    "\n",
    "    # implement __len__\n",
    "    def __len__(self): # dunder method\n",
    "        return len(self.dataset)    \n",
    "    \n",
    "    # implement __getitem__\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset.iloc[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25364ebe",
   "metadata": {},
   "source": [
    "### 3.1.3 The collate function\n",
    "The collate function is used to tell the Pytorch DataLoader how to handle datapoints from the MNISTDataset we defined earlier and pack them into a batch. By default (i.e. no specific collate_fn is passed), the DataLoader would simply add the dataset items to an array and ensure that the array is of a certain size (the batch size). This would normally not be a problem if we were working with text data that is of a fixed length.  \n",
    "\n",
    "However, in our case, we are working with image data, and our dataset (which is essentially just a Pandas DataFrame) does not actually contain the images themselves, but filepaths to them, along with labels. For this reason, we must define a custom collate function that reads these images and their labels into memory, and returns them side-by-side so we can use them in our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268754a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from torch.utils.data import DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "860f2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Define a tensor of the same size as our image batch to store loaded images into\n",
    "    img_batch_tensor = torch.FloatTensor(len(batch), 28, 28)\n",
    "    # Define empty lists to hold items we encounter\n",
    "    img_tensors = []\n",
    "    lables = []\n",
    "    for item in batch:\n",
    "        # load a single image\n",
    "        img_tensor = load_image_tensor(filepath=f\"data/MNIST/{item.iloc[0]}\",threshold=50)\n",
    "        # put image into a list \n",
    "        img_tensors.append(img_tensor)\n",
    "        # put the same image's label into anoweek-3-loading-data.ipynbther list\n",
    "        lables.append(item.iloc[1])\n",
    "\n",
    "    # Concatenate the list of individual tensors (image_tensors) into a single Pytorch tensor (image_batch_tensor)\n",
    "    torch.cat(img_tensors, out=img_batch_tensor)\n",
    "    lables_batch_tensor = torch.LongTensor(lables)\n",
    "    # Use the label list to create a torch tensor of ints\n",
    "    return img_batch_tensor,lables_batch_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This is a convenience funtion that returns dataset splits of train, val and test according to the fractions specified in the arguments\n",
    "def load_data(data_path, batch_sz=100, train_val_test_split=[0.3, 0.2, 0.5]):\n",
    "    assert sum(train_val_test_split) == 1, \"Train, val and test fractions should sum to 1!\"  # Always a good idea to use static asserts when processing arguments that are passed in by a user!\n",
    "    # Instantiate our previously defined dataset\n",
    "    dataset = MNISTDataset(data_path)\n",
    "    # split dataset into train, val and test\n",
    "    tr_val_te = []\n",
    "    for frac in train_val_test_split:\n",
    "        actual_count = frac * len(dataset)\n",
    "        actual_count = round(actual_count)\n",
    "        tr_val_te.append(actual_count)\n",
    "    \n",
    "    train_split , val_split , test_split = random_split(dataset, tr_val_te)\n",
    "    # Use Pytorch DataLoader to load each split into memory. It's important to pass in our custom collate function, so it knows how to interpret the \n",
    "    # data and load it. num_workers tells the DataLoader how many CPU threads to use so that data can be loaded in parallel, which is faster\n",
    "    \n",
    "    # Get CPU count\n",
    "    n_cpus = mp.cpu_count() # returns number of CPU cores on this machine\n",
    "    \n",
    "    train_dl = DataLoader(dataset=train_split, batch_size=batch_sz, shuffle=True, collate_fn=custom_collate_fn, num_workers= n_cpus)          \n",
    "    \n",
    "    val_dl = DataLoader(dataset=val_split, batch_size=batch_sz, shuffle=False, collate_fn=custom_collate_fn, num_workers= n_cpus)\n",
    "    \n",
    "    test_dl = DataLoader(dataset=test_split, batch_size=batch_sz, shuffle=True, collate_fn=custom_collate_fn, num_workers= n_cpus)\n",
    "    \n",
    "    return train_dl, val_dl, test_dl\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f070e70",
   "metadata": {},
   "source": [
    "### 3.1.4  Visualising image data\n",
    "We can now extend our visualisation function to be a bit fancier and take in a batch of images to display. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1815cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(batch, ncols=4):\n",
    "    height, width = batch[0].shape\n",
    "    nrows = len(batch)//ncols # calculate the number of rows based on the number of columns needed by the user\n",
    "    \n",
    "    img_grid = (batch.reshape(nrows, ncols, height, width)\n",
    "              .swapaxes(1,2)\n",
    "              .reshape(height*nrows, width*ncols))\n",
    "\n",
    "    return img_grid\n",
    "\n",
    "\n",
    "def show_batch(batch, title=\"Image batch\", cols=4):\n",
    "    N = len(batch)\n",
    "    if N > cols:\n",
    "        assert N % cols == 0, \"Number of cols must be a multiple of N\"\n",
    "    \n",
    "    result = image_grid(batch , ncols=cols)\n",
    "    fig = plt.figure(figsize=(5., 5.))\n",
    "    plt.suptitle(f\"{title} [{int(N/cols)}x{cols}]\")\n",
    "    plt.imshow(result, cmap='gray')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e5c9a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining global path variables\n",
    "MODEL_DIR = \"./saved_models\"\n",
    "DATASET_PREFIX = \"./data/MNIST\"\n",
    "DATA_PATH = f\"{DATASET_PREFIX}/raw/mnist_dataset.csv\"\n",
    "\n",
    "row ,col = 4,4\n",
    "batch = row * col\n",
    "# Load the train, validation and test sets. Get a batch of 16 images for each set.\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH,batch_sz=batch)\n",
    "train_images, train_labels = next(iter(train_dl))\n",
    "test_images, test_labels = next(iter(test_dl))\n",
    "\n",
    "# Display the batches using the previously defined `show_batch` function\n",
    "# Show train batch with title \"Train images\"\n",
    "show_batch(train_images, title=\"Train Images\", cols= col)\n",
    "print(train_labels.reshape(row,col))\n",
    "# Show validation batch with title \"Val images\"\n",
    "\n",
    "# Show test batch with title \"Test images\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
