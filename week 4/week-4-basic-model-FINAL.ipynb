{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe29295f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kavi/.virtualenvs/aim-lab-sessions/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Get all the variables, classes and functions we defined in the previous lessons\n",
    "from vars.week_3 import *\n",
    "\n",
    "# Import new modules\n",
    "import torch.nn as nn \n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d955ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics.classification as tmcls \n",
    "\n",
    "class ClassifierMetrics(object):\n",
    "    ap: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1: float\n",
    "    acc: float\n",
    "    count: int\n",
    "\n",
    "    def __init__(self, task, n_labels, device):\n",
    "        self.task = task\n",
    "        if self.task == \"multiclass\":\n",
    "            self.ap_metric = tmcls.MulticlassAveragePrecision(num_classes=n_labels, average=None, thresholds=None).to(device)\n",
    "            self.precision_metric = tmcls.MulticlassPrecision(num_classes=n_labels).to(device)\n",
    "            self.recall_metric = tmcls.MulticlassRecall(num_classes=n_labels).to(device)\n",
    "            self.f1_metric = tmcls.MulticlassF1Score(num_classes=n_labels).to(device)\n",
    "            self.acc_metric = tmcls.MulticlassAccuracy(num_classes=n_labels).to(device)\n",
    "\n",
    "        elif self.task == \"multilabel\":\n",
    "            self.ap_metric = tmcls.MultilabelAveragePrecision(num_labels=n_labels, average=None, thresholds=None).to(device)\n",
    "            self.precision_metric = tmcls.MultilabelPrecision(num_labels=n_labels).to(device)\n",
    "            self.recall_metric = tmcls.MultilabelRecall(num_labels=n_labels).to(device)\n",
    "            self.f1_metric = tmcls.MultilabelF1Score(num_labels=n_labels).to(device)\n",
    "            self.acc_metric = tmcls.MultilabelAccuracy(task=self.task, num_labels=n_labels).to(device)\n",
    "        self.reset()\n",
    "    \n",
    "\n",
    "    def reset(self):\n",
    "        self.ap = 0\n",
    "        self.precision = 0\n",
    "        self.recall = 0\n",
    "        self.f1 = 0\n",
    "        self.acc = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, y_pred, y):\n",
    "        y = y.long()\n",
    "        self.ap += self.ap_metric(y_pred, y)\n",
    "        self.precision += self.precision_metric(y_pred, y)\n",
    "        self.recall += self.recall_metric(y_pred, y)\n",
    "        self.f1 += self.f1_metric(y_pred, y)\n",
    "        self.acc += self.acc_metric(y_pred, y)\n",
    "        self.count += 1 #y.size(0)\n",
    "\n",
    "    def calc(self, y_pred, y):\n",
    "        self.reset()\n",
    "        y = y.long()\n",
    "        self.ap = self.ap_metric(y_pred, y)\n",
    "        self.precision = self.precision_metric(y_pred, y)\n",
    "        self.recall = self.recall_metric(y_pred, y)\n",
    "        self.f1 = self.f1_metric(y_pred, y)\n",
    "\n",
    "    def avg(self):\n",
    "        self.ap = self.ap / self.count\n",
    "        self.precision = self.precision / self.count\n",
    "        self.recall = self.recall / self.count\n",
    "        self.f1 = self.f1 / self.count\n",
    "        self.acc = self.acc / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a2395",
   "metadata": {},
   "source": [
    "# 4. Defining simple models \n",
    "## 4.1 nn.Sequential\n",
    "The `Sequential` class is very simple: it accepts a sequence of neural network `Modules` as arguments and arranges them such that the output of one is automatically sent to the input of the next in line. This saves us a bit of time writing some code, but has some drawbacks, as we shall see shortly. The following is the simplest possible neural network. It consists only of an input layer, 1 hidden layer and an output layer. It is good practice to print out the summary of your network using `torchsummary.summary`. This lets you inspect your networks parameters and the input/output sizes of each layer. Interestingly, it also acts as a sort of sanity checker for your model, because it will complain if the input/output sizes of your layers aren't compatible with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0334b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 128]         100,480\n",
      "              ReLU-3                  [-1, 128]               0\n",
      "            Linear-4                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 0.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_simple_linear_net():\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),                # Input is a 2d array of pixel values, so we flatten it out to 1d\n",
    "        nn.Linear(28*28, 128),       # Input layer connects each input node to each hidden node. MNIST images are 28*28 pixels, hidden size can be anything we want\n",
    "        nn.ReLU(),                   # ReLU activation only lets a signal through if it is > 0\n",
    "        nn.Linear(128, 10)  # Output connects each node in the hidden layer to 10 output classes - the number of digits we want to classify!\n",
    "        \n",
    "    )\n",
    "\n",
    "summary(get_simple_linear_net(), input_size=(1, 28, 28), device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368199e5",
   "metadata": {},
   "source": [
    "### 4.1.1  Simple training loop\n",
    "Now that we've defined a network, we can start training it! Let's define the simplest possible training function, for which we only require the model, number of training epochs, the dataloader and an optimisation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f5ef147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, train_dl, optimiser):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_steps = len(train_dl)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.train()  # set model to training mode\n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            # Prepare data and label batches \n",
    "            batch_sz = len(image_batch)\n",
    "            output = model(image_batch)\n",
    "            # explain categorical cross-entropy loss\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "            # Zero gradients and backpropagate the losses\n",
    "            optimiser.zero_grad()\n",
    "            losses.backward()\n",
    "            optimiser.step()  # update model weights based on loss gradients\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Update the total number of correct predictions and calculate accuracy\n",
    "            # this needs some explanation, draw diagram\n",
    "            preds = torch.argmax(output, dim=1) \n",
    "            correct += int(torch.eq(preds, label_batch).sum())\n",
    "            total += batch_sz\n",
    "            minibatch_accuracy = 100 * correct / total\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "            if (batch_num + 1) % 5 == 0:\n",
    "                print(\" \" * len(msg), end='\\r')\n",
    "                msg = f'Train epoch[{epoch+1}/{epochs}], MiniBatch[{batch_num + 1}/{total_steps}], Loss: {losses.item():.5f}, Acc: {minibatch_accuracy:.5f}'\n",
    "                print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec4a08cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch[5/5], MiniBatch[390/394], Loss: 0.56701, Acc: 84.06250\r"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# Defining network hyperparameters\n",
    "epochs = 5\n",
    "batch_sz = 32\n",
    "learning_rate = 0.005\n",
    "train_dl, val_dl, test_dl = load_data(DATA_PATH, batch_sz=batch_sz)   # Creating a data split\n",
    "\n",
    "network = get_simple_linear_net()                    # Creating an instance of our network\n",
    "optim = SGD(network.parameters(), lr=learning_rate)  # Stochastic gradient descent optimiser\n",
    "train_model(network, epochs, train_dl, optim)        # Calling our training function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc68cc9",
   "metadata": {},
   "source": [
    "### 4.1.2 Debrief: Simple model with simple training loop\n",
    "At the end of the training loop, our model performs pretty well - should be around 80-90% accuracy most of the time. This is definitely better than random chance, so our model seems to have learned something about the dataset and can make good predictions. But it could be better! Before we look into improving this, there is something else that needs fixing... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d7284",
   "metadata": {},
   "source": [
    "### 4.1.3 Training device\n",
    "Something you may have noticed so far is that the training loop runs quite slowly. 5 epochs is not a very long time at all in the machine learning world and it still takes a while to complete. This because we've been asking the CPU to do all the tensor calculations needed to update the weights. This is a bad idea because GPUs are much more efficient at processing large amounts of data in parallel. You should always use a GPU to train machine learning models if one is available. Pytorch makes it very easy to detect GPU availability and transfer code you've written for a CPU to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c94b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # CUDA is a low-level toolkit that provides libraries for interacting with NVIDIA graphics cards\n",
    "print(f\"Using device: {DEVICE}\") # This should print out `Using device: cuda` if Pytorch detects a GPU on your system \n",
    "\n",
    "\n",
    "# We also need to modify our training loop to accept the device as an argument, and transfer input tensors to the GPU device\n",
    "def train_model_gpu(model, epochs, train_dl, optimiser, cls_metrics):\n",
    "    msg = \"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_steps = len(train_dl)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_losses = 0\n",
    "        cls_metrics.reset()\n",
    "        model.train()\n",
    "        for batch_num, (image_batch, label_batch) in enumerate(train_dl):\n",
    "            batch_sz = len(image_batch)\n",
    "            \n",
    "            # Transferring image and label tensors to GPU #\n",
    "            image_batch = image_batch.to(DEVICE)\n",
    "            label_batch = label_batch.to(DEVICE)\n",
    "            ###############################################\n",
    "            \n",
    "            output = model(image_batch)\n",
    "            losses = nn.CrossEntropyLoss()(output, label_batch)\n",
    "            total_losses += losses.item()\n",
    "            cls_metrics.update(output, label_batch)\n",
    "            optimiser.zero_grad()\n",
    "            losses.backward()\n",
    "            optimiser.step()  \n",
    "            \n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            correct += int(torch.eq(preds, label_batch).sum())\n",
    "            total += batch_sz\n",
    "#             minibatch_accuracy = 100 * correct / total\n",
    "\n",
    "            #### Fancy printing stuff, you can ignore this! ######\n",
    "#             if (batch_num + 1) % 5 == 0:\n",
    "#         print(cls_metrics.f1)\n",
    "        cls_metrics.avg()\n",
    "        avg_loss = total_losses / total\n",
    "        acc = correct / total\n",
    "        acc = cls_metrics.acc\n",
    "        print(\" \" * len(msg), end='\\r')\n",
    "        msg = f'Train epoch[{epoch+1}/{epochs}], Loss: {avg_loss:.5f}, Acc: {acc:.5f}, F1: {cls_metrics.f1:.5f}'\n",
    "        print (msg, end='\\r' if epoch < epochs else \"\\n\",flush=True)\n",
    "            #### Fancy printing stuff, you can ignore this! ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f635bbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch[5/5], Loss: 0.01618, Acc: 0.85630, F1: 0.84115\r"
     ]
    }
   ],
   "source": [
    "# Finally, we need to transfer our model to the device as well, and can begin training\n",
    "network = get_simple_linear_net()\n",
    "optim = SGD(network.parameters(), lr=learning_rate)\n",
    "network = network.to(DEVICE)\n",
    "cls = ClassifierMetrics(\"multiclass\", 10, DEVICE)\n",
    "train_model_gpu(network, epochs, train_dl, optim, cls)\n",
    "\n",
    "# You should see a speedup in training speed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
